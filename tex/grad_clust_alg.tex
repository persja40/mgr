\chapter{Algorytm gradientowej klasteryzacji}
\label{cha:gradient_clustering_algorithm}

\section{Estymatory jądrowe}
\label{sec:estymatory}
\subsection{Definicja}

Niech zmienna losowa $X$ będzie n-wymiarową zmienną losową o gęstości prawdopodobieństwa $f$. Estymator tej zmiennej będzie wyznaczony na podstawie próby losowej
\begin{equation}
x_1, x_2, ..., x_m
\end{equation}
będacej interpretowaną jako doświadczenie z niezależnych eksperymentów zmiennej losowej $X$.
Jądrowy estymator gęstości $\hat{f}: \mathbf{R} \to [0, \infty)$ dany jest wzorem
\begin{equation}
\label{eq:estymator}
\hat{f}(x)=\frac{1}{mh^n} \displaystyle \sum_{i=1}^{m}K(\frac{x-x_i}{h})
\end{equation}
$m \in \mathbf{N_+}$ jest licznością próby losowej, $h \in \mathbf{R_+}$ jest parametrem wygładzania, a funkcja $K$ jest jądrem spełniającym warunki: \\
gęstość funkcji jest znormalizowana
\begin{equation}
\int_{\mathbf{R^n}} K(x)dx = 1
\end{equation}
funkcja jest symetryczna
\begin{equation}
K(x) = K(-x)
\end{equation}
ma w zerze maksimum globalne
\begin{equation}
K(0) \geq K(x), x \in \mathbf{R^n}
\end{equation}

\subsection{Wybór jądra}
Czy pisać o tym?

\subsection{Błąd estymacji}
MSE(ang. mean square error), czyli błąd średniokwadratowy jest kwadratem błędu estymacji, co można zapisać jako
\begin{equation}
MSE = E({(\hat{b}-b)}^2 )
\end{equation}
Wzór można przekształcić w bardziej dogodną formę
\begin{equation}
MSE = {[E(\hat{b})-b]}^2 +V(\hat{b})
\end{equation}
jako sumę kwadratu obciążenia i jego wariancji. Pierwszy człon wskazuje odchylenie "centrów" estymatora z wartościami pomiarowymi, natomiast drugi pokazuje odchylenie względem "centrum". \\
Dla estymatora gęstości prawdopodobieństwa zmiennej n-wymiarowej wzór przyjmuje postać
\begin{equation}
MSE_x = E({[\hat{f}(x)-f(x)]}^2), x \in \mathbf{R^n}
\end{equation}
lub jak poprzednio
\begin{equation}
MSE_x = {[E(\hat{f}(x))-f(x)]}^2 + V(\hat{f}(x)), x \in \mathbf{R^n}
\end{equation}

MISE(ang. mean integrated square error), czyli scałkowany błąd średniokwadratowy otrzymuje się poprzez scałkowanie poprzednich błędów na przestrzeni $\mathbf{R^n}$:
\begin{equation}
MISE = \int_{\mathbf{R^n}} E({[\hat{f}(x)-f(x)]}^2) dx
\end{equation}
lub
\begin{equation}
\label{eq:mise}
MISE = \int_{\mathbf{R^n}} [{[E(\hat{f}(x))-f(x)]}^2 + V(\hat{f}(x))]dx
\end{equation}

Wzór \eqref{eq:mise} podobnie jak poprzednio jest sumą scałkowanego obciążenia estymatora $\hat{f}$ i jego wariancji. MISE jest cennym kryterium dla wyboru postaci jądra $K$ oraz parametru wygładzania $h$.

CZY CIĄGNĄĆ DALEJ? 

\subsection{Parametr wygładzania}

Wartość parametru wygładzania ma kluczowe znaczenie dla jakości estymatora jądrowego. W tym podrozdziale zostaną przedstawione dwie metody wyznaczania tego parametru, przy użyciu MISE - scałkowanego błędu średniokwardatowego. Parametr wygładzania wpływa jednakowo na wszystkie wymiary, w następnym podrozdziale zostanie omówiona metoda jego indywidualizacji w zależności od wymiaru UWAGA ZASTANÓW SIĘ CO PISZESZ \ref{subsec:modyfikacje_h}.

W przypadku gdy zmienna losowa X jest wielowymiarowa wartość parametru wygładzania jest wyliczana osobno dla każdego wymiaru jak dla zmiennych jednowymiarowych. 

W odróżnieniu od problemu wyboru jądra dobór odpowiedniego parametru wygładzania ma kluczowe znaczenie dla jakości estymacji. Jego zbyt mała wartość skutkuje powstaniem wielu ekstremów lokalnych niemających odzwierciedlenia w danych pomiarowych. Można to również zaobserwować ze wzoru na estymator jądrowy \ref{eq:estymator}, w którym parametr wygładzania znajduje się w mianowniku przed sumą. W takiej sytuacji wzmocnione zostaną wartości znajdujące się blisko "centrum", natomiast bardziej odległe zostaną wytłumione.

W przeciwnym przypadku, gdy omawiany parametr zostanie dobrany zbyt duży, doprowadzi to do nadmiernego wygładzenia estymatora. Stłumienie "centrum" i wzmocnienie "ogonów" spowoduję zanik cech danych pomiarowych, uniemożliwiając ich odróżnienie.

DODAJ RYSUNKI dla h małe, duże i OK

INTERPRETACJA 2 z obciążeniem i wariancją

Dwie najpopularniejsze metody wyboru parametru wygładzania to :
\begin{itemize}
\item metoda podstawień(ang. plug-in method) - o mniejszym złożeniu obliczeniowym, lecz może być wykorzystany jedynie do dla danych jednowymiarowych
\item metoda krzyżowego uwiarygodnienia(ang. corss validation method) - o większej złożoności obliczeniowej, natomiast nie jest zależna od wymiarowości danych
\end{itemize}
dzięki czemu mogą być wykorzystywane komplementarnie. Obie mogą być wykorzystane w przypadku jednowymiarowym uzyskując zbliżone rezultaty ze względu na minimalizację wspólnego błędu, jakim jest błąd średniokwadratowy MISE. REF

Zakładając jednowymiarową zmienna losową, zostanie teraz przedstawiona metoda podstawień.

NAPISZ

CROSS VALIDATION

\begin{equation}
g(h)=\frac{1}{m^2h^n}\displaystyle\sum_{i=1}^{m} \displaystyle\sum_{j=1}^{m} \widetilde{K}(\frac{x_j - x_i}{h}) + \frac{2}{mh^n}K(0)
\end{equation}

\begin{equation}
G(x) = K^{*2}(x) - 2K(x)
\end{equation}

\begin{equation}
K^{*2}(x) = \int_{\mathbf{R^n}}K(y)K(x-y)dy
\end{equation}

\begin{equation}
K^{*2}(x) = (4\pi)^{-n/2} exp(-\frac{1}{4}x^Tx)
\end{equation}

W celu minimalizacji funkcji g powszechnie używana jest metoda złotego podziału, która zostanie zaprezentowana poniżej. Na wstępie należy założyć przedział poszukiwań $[a_0, b_0]$, unimodalność funkcji na przedziale oraz wyznaczyć
\begin{equation}
x^l_{0} = b_0 - \frac{\sqrt{5} - 1}{2}(b_0 - a_0)
\end{equation}

\begin{equation}
x^p_{0} = a_0 + \frac{\sqrt{5} - 1}{2}(b_0 - a_0)
\end{equation}
oraz 
\begin{equation}
f(x^l_{0})
\end{equation}
\begin{equation}
f(x^r_{0})
\end{equation}
w każdym następnym kroku dla $k \in \mathbf{N}\{0\}$ dopóki $b-a > \epsilon$ oznacza się
\begin{itemize}
\item jeśli $f(x^l_{k-1}) \leq f(x^r_{k-1}) $ \newline
	\begin{equation}
	a_k = a_{k-1}
	\end{equation}
	\begin{equation}
	b_k = x^r_{k-1}
	\end{equation}
	\begin{equation}
	x^l_k = b_k - \frac{\sqrt{5} - 1}{2}(b_k - a_k)
	\end{equation}
	\begin{equation}
	x^r_k = x^l_{k-1}
	\end{equation}
	oblicza się również wartość funkcji w punkcie 
	\begin{equation}
	f(x^l_k)
	\end{equation}
	oraz podstawia
	\begin{equation}
	f(x^r_k) = f(x^l_{k-1})
	\end{equation}
\item natomiast jeśli $f(x^l_{k-1}) > f(x^r_{k-1}) $ \newline
	\begin{equation}
	a_k = x^l_{k-1}
	\end{equation}
	\begin{equation}
	b_k = b_{k-1}
	\end{equation}
	\begin{equation}
	x^l_k = x^r_{k-1}
	\end{equation}
	\begin{equation}
	x^r_k = a_k + \frac{\sqrt{5} - 1}{2}(b_k - a_k)
	\end{equation}
	stosuje się podstawienie 
	\begin{equation}
	f(x^l_k) = f(x^r_{k-1})
	\end{equation}
	oraz wyzanacza wartość funkcji w punkcie
	\begin{equation}
	f(x^r_k)
	\end{equation}
\end{itemize}
SPRAWDŹ CZY NIE MA LITERÓWEK !?!?!!?
W momencie osiągnięcia warunku zakończenia $b-a< \epsilon$ za minimum przyjmuje się
\begin{itemize}
\item jeśli $f(x^l_{k-1}) \leq f(x^r_{k-1}) $
	\begin{equation}
	h = \frac{a_k+x^r_k}{2}
	\end{equation}
\item lecz gdy $f(x^l_{k-1}) > f(x^r_{k-1}) $
	\begin{equation}
	h = \frac{x^l_k + b_k}{2}
	\end{equation}
\end{itemize}
Koncepcyjnie metoda złotego podziału polega na stworzeniu zstępującego ciągu, w którym każdy kolejny przedział wynosi $\frac{\sqrt{5} - 1}{2} \approx 0.618$ poprzedniego przedziału. Błąd wyznaczenia minimum funkcji wynosi
\begin{equation}
\Delta = \frac{1}{2}(b_k - a_k) = \frac{1}{2} {(\frac{\sqrt{5} - 1}{2})}^{k+1}(b_0-a_0)
\end{equation}

\subsection{Modyfikacja parametru wygładzania}
\label{subsec:modyfikacje_h}
Parametr wygładzania $h$ jest jednakowy dla wszystkich jąder $K$, dlatego wprowadza się jego modyfikację. Skutkuje to możliwością zmniejszenia parametru wygładzania (wyszczuplenie jądra - wzmocnienie) w miejscach o dużej gęstości próby losowej, natomiast w rejonach mniejszej gęstości próby losowej zwiększenie wartości parametru wygładzania (rozciągnięcie jądra - osłabienie).

Metoda wyznaczenia parametrów modyfikujących $s_i>0, i \in [1, m]$ wygląda następująco :
\begin{equation}
s_i= {(\frac{\hat{f}(x_i)}{\tilde{s}})}^{-c}
\end{equation}
gdzie $c \geq 0$, $\tilde{s}$ jest średnią geometryczną ciągu $\hat{f}(x_1), \hat{f}(x_1), ..., \hat{f}(x_m)$ co można zapisać jako
\begin{equation}
\tilde{s} = exp(\frac{1}{m} \displaystyle\sum_{i=1}^{m} ln(\hat{f}(x_i)) )
\end{equation} 
Finalnie estymator jądrowy ze zmodyfikowanym parametrem wygładzania definiuje się jako:
\begin{equation}
\label{eq:est_mod}
\hat{f}(x)=\frac{1}{mh^n} \displaystyle \sum_{i=1}^{m} \frac{1}{s_i^n} K(\frac{x-x_i}{hs_i})
\end{equation}

\subsection{Liczność próby losowej}

\subsection{Przykład}

TEST CYTATÓW
\cite{Kul10}
\cite{Kul05}
\cite{Kul12}


%------------------------------------------------------------------------------------------------------

\section{Algorytm}
\label{sec:algorytm}
\subsection{Schemat}
\label{subsec:schemat}
Jako dane wejściowe przyjęty będzie m-elementowy zbiór n-elementowych wektorów, będzie on traktowany jak zbiór pomiarowy n-wymiarowej zmiennej losowej X o gęstości f. UZUPEŁNIĆ
Schemat algorytmu przedstawia się następująco:
\begin{enumerate}
\item sformułowanie jądra estymatora $\hat{f}$
\item wybór warunku zakończenia ( oraz liczby kroków, która z niego wynika)
\item procedura tworzenia klastrów i przydzielania do nich elementów
\end{enumerate}

\subsection{Sformułowanie jądra estymatora}
\label{subsec:jadro_estymatora}
Użyte zostanie estymator jądrowy ze zmodyfikowanym parametrem wygładzania \ref{eq:est_mod} z jądrem normalnym REF!?!?!?.

\subsection{Warunek zakończenia}
\label{subsec:stop_cond}
Zakłada się że klastry powiązane są z maksimami funkcji $\hat{
f}$, dlatego elementy wejściowe będą przesuwane o $\Delta\hat{f}$. Będzie to wykonywane iteracyjnie jako:
\begin{equation}
x^0_j=x_j, j\in [1,m]
\end{equation}
\begin{equation}
x^{k+1}_j=x^k_j+b\frac{\Delta\hat{f}(x^k_j)}{\hat{f}(x^k_j)}, j \in [1,m], k \in [0,k^*]
\end{equation}
gdzie $b > 0$ i $k^* \in \mathbf{N} \backslash \{0\}$. Rekomenduje się aby
\begin{equation}
b=\frac{h^2}{n+2}
\end{equation} 

Warunkiem zakończenia jest:
\begin{equation} \label{eq:stop_cond}
|D_k-D_{k-1}| \leq \alpha D_0
\end{equation}
gdzie $\alpha>0$ i 
\begin{equation}
D_0 = \displaystyle\sum_{i=1}^{m-1} \displaystyle\sum_{j=i+1}^{m} d(x_i, x_j)
\end{equation}
\begin{equation}
D_{k-1} = \displaystyle\sum_{i=1}^{m-1} \displaystyle\sum_{j=i+1}^{m} d(x_i^{k-1}, x_j^{k-1})
\end{equation}
\begin{equation}
D_{k} = \displaystyle\sum_{i=1}^{m-1} \displaystyle\sum_{j=i+1}^{m} d(x_i^{k}, x_j^{k})
\end{equation}
gdzie $d$ jest euklidesową metryką w $\mathbf{R}^n$. $D$ oznacza sumę odległości pomiędzy każdą parą elementów. Najczęściej przyjmuje się że
\begin{equation}
a= 0.0001
\end{equation}
W momencie spełnienia warunku \ref{eq:stop_cond} przyjmuje się, że obecna wartość $k$ jest ostatnią.
\begin{equation}
k^*=k
\end{equation}

\subsection{Procedura tworzenia klastrów i przydzielania do nich elementów}
\label{subsec:tworzenie_klastrow}
Z poprzedniego podrozdziału dany jest zbiór
\begin{equation} \label{eq:set_init}
x^{k^*}_1, x^{k^*}_2, ..., x^{k^*}_m 
\end{equation}
Na jego podstawie tworzony jest zbiór odległości pomiędzy każdą parą
\begin{equation} \label{eq:set_x_k}
\{ d(x^{k^*}_i, x^{k^*}_j) \}, i \in [1, m-1], j \in [i+1,m]
\end{equation}
którego liczba elementów wynosi
\begin{equation}
m_d=\frac{m(m-1)}{2}
\end{equation}
Zbiór należy potraktować jako pomiary jednowymiarowej zmiennej losowej, na podstawie którego powstanie estymator $\hat{f}_d$.

Kolejnym etapem algorytmu jest znalezienie pierwszego lokalnego minimum funkcji $\hat{f}_d$ należącego do przedziału $(0, D)$, gdzie
\begin{equation}
D = max_{i=1, ..., m-1, j=i+1, ..., m } d(x_i,x_j)
\end{equation}
Następnie ze zbiór \ref{eq:set_x_k} wyznaczyć jego odchylenie standardowe $\sigma_d$ oraz przyjąć że $x$ należy do zbioru
\begin{equation} \label{eq:set_sigma_d}
\{0.01 \sigma_d , 0.02 \sigma_d , ..., ( \lfloor 100D \rfloor -1) 0.01 \sigma_d \}
\end{equation}
Wartość $x_d$ zostanie wyznaczona jako najmniejszy element zbioru \ref{eq:set_sigma_d} spełniający warunek
\begin{equation}
\hat{f}_d(x-0.01\sigma_d) > \hat{f}_d(x) i \hat{f}_d(x) \geq \hat{f}_d(x+0.01\sigma_d) 
\end{equation}
co będzie iterpretowane jako połowa odległości pomiędzy centrami klastrów.


Zbiór \ref{eq:set_init} zostanie użyty do klasteryzacji w następujący sposób:
\begin{enumerate}
\item weź element zbioru i utwórz z niego jednoelementowy klaster
\item znajdź element zbioru bliższy niż $x_d$ i dodaj go do klastru, powtarzaj dopóki istnieją
\item klaster uznaje się za wypełniony i jego elementy zostają usunięte ze zbioru, jeśli istnieją elementy w zbiorze należy powrócić do punktu 1, w przeciwnym razie algorytm się zakończył
\end{enumerate}

Omówienie czy poszczególne fragmenty algorytmu można zrównoleglić i w jaki sposób znajdują się w kolejnym rozdziale.