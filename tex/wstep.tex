\chapter{Wstęp}
\label{cha:wstep}

W tym rozdziale zostaną zaprezentowane metody i technologie konkurencyjne dla tematu tej pracy.

\section{Klasteryzacja}
\label{sec:klasteryzacja}

\subsection{Rys historyczny}
Początki analizy skupień sięgają roku 1932, kiedy to klasteryzacja została opisana w odniesieniu do antropologi \cite{Dri32}, skąd następnie przeniknęła do psychologii. W latach 60-tych nastąpił rozwój eksploracji danych (w tamtym czasie używano nazwy ang.data fishing) na bazie metod statystycznych operujących na stosunkowo niewielkich zbiorach danych. Wzrost dostępności komputerów spowodował olbrzymi wzrost liczności zbiorów, z którymi musiano sobie poradzić. Obecnie analiza skupień jest używana m.in. do rozróżniania tkanek na prześwietleniach, grupowania klientów, znajdywania społeczności, segmentacji obrazów, systemów rekomendacji, detekcji anomalii, lokalizacji miejsc o zagrożeniu pożarowym lub bezpieczeństwa czy śledzenia obiektów.

\subsection{Algorytm k-średnich}
Algorytm k-średnich został wprowadzony przez Jamesa MacQueena \cite{Mac67} na postawie idei żydowskiego matematyka polskiego pochodzenia Hugo Steinshausa z lwowskiej szkoły matematycznej.\newline
Ma on za zadanie przypisać n elementów zbioru do k klastrów, gdzie k jest znane. Jest to problem NP-trudny, dlatego metoda korzysta z heurystyki, a przedstawia się ona następująco:
\begin{enumerate}
	\item{załóż liczbę klastrów k}
	\item{wylosuj środki klastrów $\mu_i$}
	\item{przypisz elementy zbioru do klastrów o najbliższym środku}
	\item{wylicz nowe środki klastrów jako średnie elementów w skupieniu}
	\item{wróć do punktu 3 jeśli nastąpiła zmiana przynależności do klastrów w punkcie 4}
\end{enumerate}
Niestety algorytm posiada kilka wad. Brak determinizmu wynika bezpośredniego z punktu 2, gdzie środki klastrów są losowane. Będzie to wpływać na przynależność szczególnie elementów znajdujących na brzegach klastrów. Kolejne problemy mogą wynikać z różnej liczności klastrów, różnych gęstości czy nieregularnych kształtów co pokazuje dodatek \ref{sec:k_means}.

\subsection{Algorytmy hierarchiczne}
Celem tej grupy jest budowa hierarchicznych klastrów, które można przedstawić za pomocą dendrogramu. Metody z tej kategorii dzielą się na 2 główne grupy:
\begin{itemize}
	\item aglomeracyjne (bottom-up) - każdy element tworzy własny klaster, które to są łączone w wyższych poziomach hierarchii
	\item deglomeracyjne/podziałowe (top-down) - początek stanowi jeden klaster, dzieląc się w miarę schodzenia w poziomach drzewa
\end{itemize}
Głównymi różnicami pomiędzy implementacjami tych metod jest metryka i sposób łączenia. Najpopularniejsza okazała się metoda aglomeracyjna, lecz posiada złożoność czasową $\mathcal{O}(n^3)$ i pamięciową $\mathcal{O}(n^2)$.

\subsection{Algorytmy gęstościowe DBSCAN}
Density-based spatial clustering of applications with noise jest najmłodszym z prezentowanych algorytmów a zarazem jednym z najczęściej cytowanej i najbardziej popularnych \cite{Est96}.
Podstawowe pojęcia:
\begin{itemize}
	\item punkt rdzeniowy (ang. core point) - punkt w otoczeniu $\epsilon$ posiada co najmniej $minPts$
	\item punkt brzegowy (ang. border point) - punkt nie jest rdzeniowy, lecz leży w otoczenie $\epsilon$ punktu rdzeniowego
	\item punkt szumu (ang. noise point) - punkt nie jest rdzeniowy ani brzegowy
\end{itemize}
Abstrakt algorytmu przedstawia się następująco: \cite{Sch17}
\begin{enumerate}
	\item wyznacz sąsiadów dla każdego punktu w otoczeniu $\epsilon$ i oceń czy jest rdzeniowy
	\item złącz punkty rdzeniowe w otoczeniu $\epsilon$ w klastry
	\item dla każdego nierdzeniowego punkty oznacz go jako punkt brzegowy lub punkt szumu
\end{enumerate}
Złożoność czasowa algorytmu w przypadku pesymistycznym wynosi $\mathcal{O}(n^2)$ co jest akceptowalne. Do wad należy zaliczyć niedeterministyczność w klasteryzacji punktów brzegowych, ponieważ przydzielenie ich może zależeć od kolejności ułożenia danych. Dla wielowymiarowych danych metryki odległościowe mogą prowadzić do tzw. ``przekleństwa wielowymiarowości`` co znacząco utrudnia dobór $\epsilon$, co więcej wybór otoczenia nie jest trywialny gdy brak specjalistycznej wiedzy na temat danych. Dostosowanie minimalnej liczby sąsiadów stanowiącej punkt rdzeniowy jest trudne, gdy klastry znacząco różnią się gęstością.

\subsection{Sposoby oceny klasteryzacji}
INDEKS RANDA, CLUSTER COHESION, CLUSTER SEPARATION, DAVIES-BOULDIN, DUNN

\section{Akceleracja GPU}
\label{sec:akceleracja}
General-purpose computing on graphics processing units (GPGPU) polega na wykorzystaniu procesorów graficznych do wykonania obliczeń typowo obsługiwanych przez procesor. Rozwój tych technologi zapoczątkował przełom w czasie wykonywania obliczeń dla wielu problemów, w których obecnie wykorzystanie procesorów graficznych staję się standardem jak chociażby głębokie uczenie, obróbka chmur danych, renderowanie scen czy ostatnio śledzenie promieni.

\subsection{Wprowadzenie do programowania GPU}
Z punktu widzenia procesora, karty graficzne są zewnętrznymi urządzeniami wejścia-wyjścia, dlatego istotną kwestią będzie przesył danych. Główną pamięcia procesora (pomijając pamięci cache) będzie RAM (ang. random access memory), natomiast w przypadku karty graficznej będzie to VRAM (ang. video random access memory). Zarówno procesor jak i akcelerator graficzny wykonują obliczenia tylko na własnej pamięci. Istnieją wysokopoziomowe interfejsy programistyczne pozwalające na dostęp do wspólnej pamięci RAM i VRAM, jak CUDA unified memory, lecz w sytuacji gdy dane nie zostaną znalezione w pamięci modułu wykonującego obliczenia mogą zostać niejawnie skopiowane. Niepotrzebne kopiowanie danych lub dostęp do innej pamięci między CPU a GPU jest stratą czasu obliczeniowego, gdzie w niektórych przypadkach dostęp do danych stanowi główną składaową czasu działania programu. Rdzenie procesora są bardziej zaawansowanymi i bardziej ogólnymi układami niż procesory graficzne. Programowanie GPU polega na przydzieleniu wspólnych danych grupie procesorów, gdzie każdy z nich wykonuje część obliczeń (ang. stream processing). Z tego powodu GPGPU nie jest użyteczne w problemach stricte senkwencyjnych oraz gdy wymagane są zaawansowane instrukcje procesora. Główną przewagą GPU nad procesorami jest kilkadziesiąt razy większa moc obliczeniowa uzyskana za zbliżoną cenę. ZNAJDŹ JAKIEŚ WYKRESY, WZROST WYDAJNOŚCI W CZASIE, CENY, SPECYFIKACJIA KART Z FP64 I FP32, PROMOTOR: ODNIESIENIE DO ŹRÓÐŁA. Mówiąc o programowaniu GPU należy wspomnieć o 3 głównych typach jednostek są to :
\begin{itemize}
	\item \it{FP16} (ang. floating point 16 bit) - jendostki połowicznej precyzji, najczęściej wykorzystywane przy uczeniu maszynowym i głębokim uczeniu, gdzie precyzja liczb zmiennoprzecinkowych nie jest krytyczna. Dostępne w nowszych kartach graficznych.
	\item \it{FP32} (ang. floating point 32 bit) - jednostki pojedynczej precyzji, najbardziej popularne.
	\item \it{FP64} (ang. floating point 64 bit) - jednostki podwójnej precyzji, ilość ich znacząco różni się w segmencie konsumenckim i profesjonalnym (stodunek do FP32 to często odpowiednio 1/32 i 1/2). Wykorzystywane głównie w problemach inżynierskich, gdzie wymagana jest precyzja obliczeń jak metoda elementów skończonych czy modelowanie 3D.
\end{itemize}
Technologia CUDA od wersji 8 pozwala na wykonywanie 2 identycznych operacji niższej precyzji na jednostkach wyższej precyzji. \cite{Mix16}

Schemat obliczeń na akceleratorez graficznym wygląda następująco:
\begin{enumerate}
	\item przesłanie danych z RAM do VRAM
	\item wykonanie obliczeń po stronie GPU
	\item przesłanie danych z VRAM do RAM
\end{enumerate}

Może on zostać zrealizowany przy użyciu 2 głównych środowisk OpenCL i CUDA.

\subsection{OpenCL}


































 