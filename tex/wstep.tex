\chapter{Wstęp}
\label{cha:wstep}

W tym rozdziale zostaną zaprezentowane metody i technologie konkurencyjne dla tematu tej pracy.

\section{Klasteryzacja}
\label{sec:klasteryzacja}

\subsection{Rys historyczny}
Początki analizy skupień sięgają roku 1932, kiedy to klasteryzacja została opisana w odniesieniu do antropologi \cite{Dri32}, skąd następnie przeniknęła do psychologii. W latach 60-tych nastąpił rozwój eksploracji danych (w tamtym czasie używano nazwy ang.data fishing) na bazie metod statystycznych operujących na stosunkowo niewielkich zbiorach danych. Wzrost dostępności komputerów spowodował olbrzymi wzrost liczności zbiorów, z którymi musiano sobie poradzić. Obecnie analiza skupień jest używana m.in. do rozróżniania tkanek na prześwietleniach, grupowania klientów, znajdywania społeczności, segmentacji obrazów, systemów rekomendacji, detekcji anomalii, lokalizacji miejsc o zagrożeniu pożarowym lub bezpieczeństwa czy śledzenia obiektów.

\subsection{Algorytm k-średnich}
Algorytm k-średnich został wprowadzony przez Jamesa MacQueena \cite{Mac67} na postawie idei żydowskiego matematyka polskiego pochodzenia Hugo Steinshausa z lwowskiej szkoły matematycznej.\newline
Ma on za zadanie przypisać n elementów zbioru do k klastrów, gdzie k jest znane. Jest to problem NP-trudny, dlatego metoda korzysta z heurystyki, a przedstawia się ona następująco:
\begin{enumerate}
	\item{załóż liczbę klastrów k}
	\item{wylosuj środki klastrów $\mu_i$}
	\item{przypisz elementy zbioru do klastrów o najbliższym środku}
	\item{wylicz nowe środki klastrów jako średnie elementów w skupieniu}
	\item{wróć do punktu 3 jeśli nastąpiła zmiana przynależności do klastrów w punkcie 4}
\end{enumerate}
Niestety algorytm posiada kilka wad. Brak determinizmu wynika bezpośredniego z punktu 2, gdzie środki klastrów są losowane. Będzie to wpływać na przynależność szczególnie elementów znajdujących na brzegach klastrów. Kolejne problemy mogą wynikać z różnej liczności klastrów, różnych gęstości czy nieregularnych kształtów co pokazuje dodatek \ref{sec:k_means}.

\subsection{Algorytmy hierarchiczne}
Celem tej grupy jest budowa hierarchicznych klastrów, które można przedstawić za pomocą dendrogramu. Metody z tej kategorii dzielą się na 2 główne grupy:
\begin{itemize}
	\item aglomeracyjne (bottom-up) - każdy element tworzy własny klaster, które to są łączone w wyższych poziomach hierarchii
	\item deglomeracyjne/podziałowe (top-down) - początek stanowi jeden klaster, dzieląc się w miarę schodzenia w poziomach drzewa
\end{itemize}
Głównymi różnicami pomiędzy implementacjami tych metod jest metryka i sposób łączenia. Najpopularniejsza okazała się metoda aglomeracyjna, lecz posiada złożoność czasową $\mathcal{O}(n^3)$ i pamięciową $\mathcal{O}(n^2)$.

\subsection{Algorytmy gęstościowe DBSCAN}
Density-based spatial clustering of applications with noise jest najmłodszym z prezentowanych algorytmów a zarazem jednym z najczęściej cytowanej i najbardziej popularnych \cite{Est96}.
Podstawowe pojęcia:
\begin{itemize}
	\item punkt rdzeniowy (ang. core point) - punkt w otoczeniu $\epsilon$ posiada co najmniej $minPts$
	\item punkt brzegowy (ang. border point) - punkt nie jest rdzeniowy, lecz leży w otoczenie $\epsilon$ punktu rdzeniowego
	\item punkt szumu (ang. noise point) - punkt nie jest rdzeniowy ani brzegowy
\end{itemize}
Abstrakt algorytmu przedstawia się następująco: \cite{Sch17}
\begin{enumerate}
	\item wyznacz sąsiadów dla każdego punktu w otoczeniu $\epsilon$ i oceń czy jest rdzeniowy
	\item złącz punkty rdzeniowe w otoczeniu $\epsilon$ w klastry
	\item dla każdego nierdzeniowego punkty oznacz go jako punkt brzegowy lub punkt szumu
\end{enumerate}
Złożoność czasowa algorytmu w przypadku pesymistycznym wynosi $\mathcal{O}(n^2)$ co jest akceptowalne. Do wad należy zaliczyć niedeterministyczność w klasteryzacji punktów brzegowych, ponieważ przydzielenie ich może zależeć od kolejności ułożenia danych. Dla wielowymiarowych danych metryki odległościowe mogą prowadzić do tzw. ``przekleństwa wielowymiarowości`` co znacząco utrudnia dobór $\epsilon$, co więcej wybór otoczenia nie jest trywialny gdy brak specjalistycznej wiedzy na temat danych. Dostosowanie minimalnej liczby sąsiadów stanowiącej punkt rdzeniowy jest trudne, gdy klastry znacząco różnią się gęstością.

\subsection{Sposoby oceny klasteryzacji}

\subsubsection{Indeks Randa}
W przypadku porównywania klasteryzacji z wzrocową, bądź przy porównaniu dwóch rozwiązać można użyć indeksu Randa danego wzorem
\begin{equation}
R=\frac{a+b}{a+b+c+d}=\frac{a+b}{\binom{n}{2}},
\end{equation}
gdzie
\begin{itemize}
\item $a$ - liczba par należących do takich samych klastrów obu przypadkach
\item $b$ - liczba par należących do różnych klastrów w obu przypadkach
\item $c$ - liczba par należących do takich samych klastrów w pierwszych przypadku i należących do różnych klastrów w drugim przydziale
\item $d$ - liczba par należących do różnych klastrów w pierwszych przypadku i należących do takich samych klastrów w drugim przydziale
\item $n$ - liczba elementów zbioru
\end{itemize}

\subsubsection{Spójność wewnątrzklastrowa i rozrzut międzyklastrowy}
Spójność wewnątrzklastrowa (ang. cluster cohesion) jest miarą określającą jak podobne są elementy we wspólnym klastrze, a definiowana jest jako
\begin{equation}
CC = \displaystyle \sum_{i=1}^{m} d(x_i, \mu_i)^2.
\end{equation}
Rorzut międzyklastrowy (ang. cluster separation) jest miarą określającą jak niepodobny jest klaster w stosunku do innych, a przedstawia się ją jako
\begin{equation}
CS = \displaystyle \sum_{i=1}{k} m_i d(\mu_i, \mu)^2,
\end{equation}
gdzie
\begin{itemize}
\item $d$ - funkcja odległości między elementami
\item $m$ - ilość elementów klastru
\item $k$ - ilość klastrów
\item $\mu$ - centrum klastru
\end{itemize}
co ciekawe
\begin{equation}
CC + CS = const.
\end{equation}

\subsubsection{Indeks Daviesa-Bouldina}
Indeks Daviesa-Bouldina bazuje na spójności wewnątrzklastrowej $S$ oraz rorzucie międzyklastrowym $M$ \cite{DB79}. Niech $R$ będzie ogólną miarą rozdzielności klastrów mającą własności:
\begin{enumerate}
\item \begin{equation}
R(S_i,S_j,M_{ij}) \geq 0
\end{equation}
\item \begin{equation}
R(S_i,S_j,M_{ij}) = R(S_j,S_i,M_{ji})
\end{equation}
\item \begin{equation}
R(S_i,S_j,M_{ij}) = 0\text{, jeśli } S_i=S_j=0
\end{equation}
\item \begin{equation}
\text{jeśli } S_j=S_k \text{ oraz } M_{ij} < M_{ik} \text{ wtedy } R(S_i,S_j,M_{ij}) > R(S_i,S_k,M_{ik})
\end{equation}
\item \begin{equation}
\text{jeśli } M_{ij} = M_{ik} \text{ oraz } S_j > S_k \text{ wtedy } R(S_i,S_j,M_{ij}) > R(S_i,S_k,M_{ik})
\end{equation}
\end{enumerate}
Proponowana jest poniższa funkcja \cite{DB79}
\begin{equation}
R_{ij} \equiv \frac{S_i + S_j}{M_{ij}}
\end{equation}
Dla takiej funkcji indeks oceny klasteryzacji dany jest jako:
\begin{equation}
\bar{R}_{ij} \equiv \frac{i}{N} \displaystyle \sum_{i=1}^{N} R_i \text{, gdzie } R_{ij} \equiv \text{ max } R_{ij} \text{ dla } i \neq j
\end{equation}
czyli średnia podobieństw między każdym klastrem a jemu najbardziej podobnym. Minimalizacja indeksu powinna prowadzić do \say{lepszej} klasteryzacji.

\subsubsection{Indeks Dunna}
Indeks Dunna został zaprojektowany, aby określać klastry o małej wariancji między elementami oraz separowalność klastrów \cite{Dunn73}. Można go zapisać jako
\begin{equation}
D = \frac{\min_{1 \leq q \leq r \leq k} \delta(C_q, C_r) }{\max_{1 \leq p \leq k} \Delta_p},
\end{equation}
gdzie
\begin{itemize}
\item $k$ - liczba klastrów
\item $\delta$ - odległość międzyklastrowa
\item $\Delta$ - odległość wewnątrzklastrowa
\end{itemize}
Maksymalizacja indeksu prowadzi do \say{lepszej} klasteryzacji.

\section{Akceleracja GPU}
\label{sec:akceleracja}
General-purpose computing on graphics processing units (GPGPU) polega na wykorzystaniu procesorów graficznych do wykonania obliczeń typowo obsługiwanych przez procesor. Rozwój tych technologi zapoczątkował przełom w czasie wykonywania obliczeń dla wielu problemów, w których obecnie wykorzystanie procesorów graficznych staję się standardem jak chociażby głębokie uczenie, obróbka chmur danych, renderowanie scen czy ostatnio śledzenie promieni.

\subsection{Wprowadzenie do programowania GPU}
Z punktu widzenia procesora, karty graficzne są zewnętrznymi urządzeniami wejścia-wyjścia, dlatego istotną kwestią będzie przesył danych. Główną pamięcia procesora (pomijając pamięci cache) będzie RAM (ang. random access memory), natomiast w przypadku karty graficznej będzie to VRAM (ang. video random access memory). Zarówno procesor jak i akcelerator graficzny wykonują obliczenia tylko na własnej pamięci. Istnieją wysokopoziomowe interfejsy programistyczne pozwalające na dostęp do wspólnej pamięci RAM i VRAM, jak CUDA unified memory, lecz w sytuacji gdy dane nie zostaną znalezione w pamięci modułu wykonującego obliczenia mogą zostać niejawnie skopiowane. Niepotrzebne kopiowanie danych lub dostęp do innej pamięci między CPU a GPU jest stratą czasu obliczeniowego, gdzie w niektórych przypadkach dostęp do danych stanowi główną składaową czasu działania programu. Rdzenie procesora są bardziej zaawansowanymi i bardziej ogólnymi układami niż procesory graficzne. Programowanie GPU polega na przydzieleniu wspólnych danych grupie procesorów, gdzie każdy z nich wykonuje część obliczeń (ang. stream processing). Z tego powodu GPGPU nie jest użyteczne w problemach stricte senkwencyjnych oraz gdy wymagane są zaawansowane instrukcje procesora. Główną przewagą GPU nad procesorami jest kilkadziesiąt razy większa moc obliczeniowa uzyskana za zbliżoną cenę. ZNAJDŹ JAKIEŚ WYKRESY, WZROST WYDAJNOŚCI W CZASIE, CENY, SPECYFIKACJIA KART Z FP64 I FP32, PROMOTOR: ODNIESIENIE DO ŹRÓÐŁA. Mówiąc o programowaniu GPU należy wspomnieć o 3 głównych typach jednostek są to :
\begin{itemize}
	\item \it{FP16} (ang. floating point 16 bit) - jendostki połowicznej precyzji, najczęściej wykorzystywane przy uczeniu maszynowym i głębokim uczeniu, gdzie precyzja liczb zmiennoprzecinkowych nie jest krytyczna. Dostępne w nowszych kartach graficznych.
	\item \it{FP32} (ang. floating point 32 bit) - jednostki pojedynczej precyzji, najbardziej popularne.
	\item \it{FP64} (ang. floating point 64 bit) - jednostki podwójnej precyzji, ilość ich znacząco różni się w segmencie konsumenckim i profesjonalnym (stodunek do FP32 to często odpowiednio 1/32 i 1/2). Wykorzystywane głównie w problemach inżynierskich, gdzie wymagana jest precyzja obliczeń jak metoda elementów skończonych czy modelowanie 3D.
\end{itemize}
Technologia CUDA od wersji 8 pozwala na wykonywanie 2 identycznych operacji niższej precyzji na jednostkach wyższej precyzji. \cite{Mix16}

Schemat obliczeń na akceleratorez graficznym wygląda następująco:
\begin{enumerate}
	\item przesłanie danych z RAM do VRAM
	\item wykonanie obliczeń po stronie GPU
	\item przesłanie danych z VRAM do RAM
\end{enumerate}

Może on zostać zrealizowany przy użyciu 2 głównych środowisk OpenCL i CUDA.

\subsection{OpenCL}


































 